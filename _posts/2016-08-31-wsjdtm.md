---
layout: post
title:  "Parsing DTMs from the Wall Street Journal"
date:   2016-08-31
categories: post
---

A longstanding interest of mine is how investors respond to political instability -- like news of corruption scandals, gridlock, etc. Since the Wall Street Journal is a good financial newspaper of record, I'm wondering if the content of WSJ articles can help predict fluctuations in bonds and equities, when combined with monthly economic indicators like industrial production, building permits, etc. 

As a first step towards finding out, I wanted to make a script to web-scrape WSJ articles about a given country to create a document term matrix (DTM). Since the WSJ is a dynamic site, we're going to have to use `selenium` here. 

Start by importing a whole bunch of `selenium` bells and whistles, along with standard ML stuff and some processing tools from `nltk`. 

{% highlight python %}
import os, re, csv, nltk, datetime
import pandas as pd
import numpy as np

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from urllib.request import urlopen
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from unidecode import unidecode

#  Import stemmers and dictionaries for stop words
snowball = nltk.stem.SnowballStemmer('english')
lancaster = nltk.stem.LancasterStemmer()
stop_words = urlopen('http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop').read().decode("utf-8").split("\n")
stop_words = set(stop_words)

{% endhighlight %}

We'll also borrow a function from <a href = "https://github.com/andrewzc/python-wsj">andrewcz</a> to help extract URLs as string from html `<a>` tags. (More on this later.) The start of Andrew's script is useful for getting acquainted with `selenium` but we'll have to go farther if we want to create DTMs for thousands of articles in a robust way.
{% highlight python %}
def getPageUrl(elementLinks):
    extractLinks = []
    for element in elementLinks:
        links = element.get_attribute('href')
        extractLinks.append(links)
    return(extractLinks)
{% endhighlight %}

To start we'll bring up the WSJ in Firefox - and I'll enter my login credentials in the placeholders `your_username` and `your_password` 

{% highlight python %}
# Loading home URL
browser = webdriver.Firefox()
browser.get('http://markets.wsj.com/?mod=Homecle_MDW_MDC')

# Login Credentials
login = browser.find_element_by_link_text("Log In").click()
loginID = browser.find_element_by_id("username").send_keys('your_username')
loginPass = browser.find_element_by_id("password").send_keys('your_password')
loginReady = browser.find_element_by_class_name("login_submit")
loginReady.submit()

{% endhighlight %}

Now that we're logged in, let's choose a country and start scraping articles about it. In our example, we'll use Malaysia, which has been the subject of a lot of interesting news over the past few years, including the 1MDB scandal and the tragic disappearance of Malaysian Airlines flight 370. Remember that we've just logged in with our credentials, so to make sure we can input search terms without the code crashing, we'll need to first invoke a wait command. Here, we're giving the page 10 seconds to load before we try to locate the search box. Once we've found it, we can enter the word 'Malaysia' into the search box through our Python script. 

{% highlight python %}

## Basic search: articles containing Malaysia 
WebDriverWait(browser, 10).until(
        EC.presence_of_element_located((By.ID, "globalHatSearchInput"))
    )

search_box = browser.find_element_by_id("globalHatSearchInput")
search_box.clear()
search_box.send_keys('Malaysia') # Input search keyword
WebDriverWait(browser, 3)
search_req = browser.find_element_by_css_selector('.button-search').click()

{% endhighlight %}

If you're using Firefox (as opposed to Chrome), there will be an annoying cookies disclaimer banner that interferes with further scraping until you close it. So let's take care of that:

{% highlight python %}

## Close cookie policy if needed
try:
    browser.find_element_by_class_name("close").click()
except NoSuchElementException:
    print('Cookie agreement already acknowledged')
    
{% endhighlight %}

Next, let's narrow our search a bit. In particular, we can focus on a specific date range and also isolate articles whose subject is Malaysia. The code here is finding the ADVANCED SEARCH link, clicking it, inputting dates, putting `Malaysia` in the Keywords box, and excluding all manner of videos, blogs etc. These are less traditional forms of media and also more difficult to scrape (although future versions of this project could try to incorporate these, too). 

{% highlight python %}
## Expand ADVANCED SEARCH and enter a date range. 
toggleMenu = browser.find_element_by_link_text("ADVANCED SEARCH")
toggleMenu.click()
menuOptions = browser.find_element_by_class_name('datePeriod')
browser.find_element_by_name("sfrom").send_keys("2014/08/25")
browser.find_element_by_name("sto").send_keys("2016/08/25")

## Restrict search to articles whose subject is the country:
browser.find_element_by_id('metadata').send_keys("Malaysia")

## Restrict search to articles only (exclude videos, blogs, etc)
browser.execute_script("window.scrollTo(0, 500)")
browser.find_element_by_link_text("WSJ Blogs").click()
browser.find_element_by_link_text("WSJ Videos").click()
browser.find_element_by_link_text("WSJ Site Search").click()

## Scroll up to the search button and click
browser.execute_script("window.scrollTo(0, 0)")
searchArchive = browser.find_element_by_class_name('keywordSearchBar')
searchArchive.find_element_by_class_name("searchButton").click()

{% endhighlight %} 

If you're following along, this should give you page 1 of 38 pages of search results -- and there should be 758 of them. We'll want to keep track of these numbers when extracting URLs from the search results, so let's define them as variables:

{% highlight python %}
pageCount = browser.find_elements_by_class_name("results-count")[1].text
pageCount = int(re.sub(r'of ', '', pageCount))
pageCount 
# 38

resultCount = browser.find_elements_by_class_name("results-count")[0].text
resultCount = int(resultCount.rpartition("of ")[2])
resultCount
# 758
{% endhighlight %}

Now we can loop through each page in the search results and extract the article links - see Andrew's script for the prototype of this:

{% highlight python %}

## Extract all article urls

articleLinks = []
for j in range(0, pageCount):
    elementLinks = browser.find_elements_by_xpath('//h3[@class="headline"]/a')
    links = getPageUrl(elementLinks)
    articleLinks.append(links)
    print('done with page ' + str(j+1) + ' of ' + str(pageCount))
    if j < (pageCount - 1):
        browser.find_element_by_class_name("next-page").click()
    
articleLinks = [y for x in articleLinks for y in x]

{% endhighlight %}

We now have a `articleLinks`. which we can consider writing to a .csv file for easier use later on, if we want to redo the analysis.

{% highlight python %}
# Write list of urls to a csv file for later use:
with open("mys_urls.csv", "w") as csvfile:
    writer = csv.writer(csvfile, delimiter= ",")
    hdr = ['articleLink']
    writer.writerow(hdr)
    for link in articleLinks:
        entry = [link] 
        writer.writerow(entry)
        
df = pd.read_csv("mys_urls.csv")
df.head()        
                                         articleLink
0  http://www.wsj.com/articles/thailands-terroris...
1  http://www.wsj.com/articles/asian-shares-mixed...
2  http://www.wsj.com/articles/photos-mother-daug...
3  http://www.wsj.com/articles/asian-shares-mixed...
4  http://www.wsj.com/articles/key-figure-in-1mdb...        
{% endhighlight %}

Now we're ready to loop through each of these URLs to extract the text of each article. Start by creating three placeholders. The first is a dictionary `Articles` where the key will be the headline of each Article as extracted from the URL. This Articles dictionary will be *nested* so that each headline then has its own inner dictionary with keys `time stamp`, `link`, and `unigrams`, which is yet another dictionary that will extract the article's tokens. We will also make an overarching `Unigrams` dictionary to store counts of every (stemmed) unigram we encounter in our scraping. Finally, we'll have a counter `article_count` to keep track of how many results we've scraped so far. 
{% highlight python %}
## Create placeholder dictionaries for Articles and Unigrams (in all articles):
Articles = {}
Unigrams = {}
article_count = 0
{% endhighlight %}

We can loop through articles as follows (here's code for just the first 99). I'll describe what it does down below. 

{% highlight python %} 
for i in df.articleLink[:100]:
    
    browser.get(i)
    
    # Get headline if it exists (otherwise continue) 
    try:
        headline = browser.find_element_by_class_name("wsj-article-headline").text
    except NoSuchElementException:
        print(i + ' : no headline')
        continue
    
    # Enter article headline into dictionary 
    Articles[headline] = {}
    
    # Get timestamp if it exists (otherwise continue)
    try:
        timestamp = browser.find_element_by_class_name("timestamp").text
    except NoSuchElementException:
        print(i + ' : no time stamp')
        continue 
    
    # Clean time stamp if it exists 
    timestamp = re.sub(r'Updated ', '', timestamp)
    timestamp = re.sub(r' ET', '', timestamp)
    timestamp = re.sub(r'p.m.', 'PM', timestamp)
    timestamp = re.sub(r'a.m.', 'AM', timestamp)
    if 'Sept.' in timestamp:
        timestamp = re.sub(r'Sept.', 'Sep.', timestamp)
    if "COMMENTS" in timestamp:
        timestamp = timestamp.split("\n")[0]
    if 'AM' in timestamp or 'PM' in timestamp:
        try:
            timestamp = datetime.datetime.strptime(timestamp, '%b. %d, %Y %I:%M %p')
        except ValueError:
            timestamp = datetime.datetime.strptime(timestamp, '%B %d, %Y %I:%M %p')
    else:
        try:
            timestamp = datetime.datetime.strptime(timestamp, '%B %d, %Y')
        except ValueError:
            timestamp = datetime.datetime.strptime(timestamp, '%b. %d, %Y')
    
    # Put timestamp and link into Article dictionary 
    Articles[headline]["date"] = timestamp
    Articles[headline]["link"] = i
        
    # Extract article text
    paragraphs = browser.find_elements_by_xpath('//*[@id="wsj-article-wrap"]/p')
    text = []
    for t in range(0, len(paragraphs)):
        if ('@wsj.com' not in paragraphs[t].text and 
            'contributed to this article' not in paragraphs[t].text):
            text.append(paragraphs[t].text)
    text = "".join(text)    
    text = re.sub(r'\n', ' ', text)
    text = re.sub("\W", " ", text)
    text = text.lower()
    # Tokenize, stem and remove standalone numbers and (some) proper nouns 
    tokens = nltk.word_tokenize(text)
    tokens = [snowball.stem(w) for w in tokens]
    tokens = [x for x in tokens if x not in stop_words]
    tokens = [x for x in tokens if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]
    tokens = [word for word,pos in pos_tag(tokens) if pos != "NNP"]
    
    # Add tokens to inner unigrams dictionary
    Articles[headline]["unigrams"] = {}
    for j in tokens:
        count = tokens.count(j)
        Articles[headline]["unigrams"][j] = count
        if j in Unigrams:
            Unigrams[j] += count
        else:
            Unigrams[j] = count
            
    article_count += 1   
{% endhighlight %}
   
The code first opens the url. We extract the headline if it exists, and enter it into the `Articles` dictionary. Some articles don't have their headline within the "wsj-article-headline" class, and so we put a continue to just keep moving forward though other links. Next we do a similar trick for finding the time stamp, again employing a continue error handling just in case. We have to do a fair amount of work to clean the timestamp format into something consistent across articles, and put it in the `Articles` dictionary.    

After we extract basic info about the article, we'll need to get its tokens and put them in two places: first, inside the `Articles' dictionary entry for the given article; second, each word will have to go into the outer `Unigrams' dictionary. To extract paragraphs of article text, we isolate the `<p>` tags, remove ending text about contributions and author contacts, and do some more tokenizing / processing. We try to remove some of the proper nouns with the NNP tag, but this doesn't work very well... definitely a point to improve in future iterations. 

Whew. Now we have all of our information in python and it's time to translate it into a DTM .csv file:
{% highlight python %}

hdr = ['headline'] + ['date'] + [unidecode(x) for x in list(Unigrams.keys())]
with open("unigrams_mys.csv", "w") as csvfile:
    writer = csv.writer(csvfile, delimiter= ",")
    writer.writerow(hdr)
    titles = list(Articles.keys())
    for i in range(0, len(titles)):
        toWrite = []
        toWrite.append(unidecode(titles[i]))
        toWrite.append(Articles[titles[i]]["date"])
        for j in list(Unigrams.keys()):
            if j in Articles[titles[i]]['unigrams']:
               toWrite.append(unidecode(str(Articles[titles[i]]["unigrams"][j])))
            else:
                toWrite.append(str(0))
        writer.writerow(toWrite)

{% endhighlight %}

