---
layout: post
title:  "Parsing DTMs from the Wall Street Journal"
date:   2016-08-31
categories: post
---

A longstanding interest of mine is how investors respond to political instability -- like news of corruption scandals, gridlock, etc. Since the Wall Street Journal is a good financial newspaper of record, I'm wondering if the content of WSJ articles can help predict fluctuations in bonds and equities, when combined with monthly economic indicators like industrial production, building permits, etc. 

As a first step towards finding out, I wanted to make a script to web-scrape WSJ articles about a given country to create a document term matrix (DTM). Since the WSJ is a dynamic site, we're going to have to use `selenium` here. 

Start by importing a whole bunch of `selenium` bells and whistles, along with standard ML stuff and some processing tools from `nltk`. 

{% highlight python %}
import os, re, csv, nltk, datetime
import pandas as pd
import numpy as np

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from urllib.request import urlopen
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from unidecode import unidecode

#  Import stemmers and dictionaries for stop words
snowball = nltk.stem.SnowballStemmer('english')
lancaster = nltk.stem.LancasterStemmer()
stop_words = urlopen('http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop').read().decode("utf-8").split("\n")
stop_words = set(stop_words)

{% endhighlight %}

We'll also borrow a function from <a href = "https://github.com/andrewzc/python-wsj">andrewcz</a> to help extract URLs as string from html `<a>` tags. (More on this later.) The start of Andrew's script is useful for getting acquainted with `selenium` but we'll have to go farther if we want to create DTMs for thousands of articles in a robust way.
{% highlight python %}
def getPageUrl(elementLinks):
    extractLinks = []
    for element in elementLinks:
        links = element.get_attribute('href')
        extractLinks.append(links)
    return(extractLinks)
{% endhighlight %}

To start we'll bring up the WSJ in Firefox - and I'll enter my login credentials in the placeholders `your_username` and `your_password` 

{% highlight python %}
# Loading home URL
browser = webdriver.Firefox()
browser.get('http://markets.wsj.com/?mod=Homecle_MDW_MDC')

# Login Credentials
login = browser.find_element_by_link_text("Log In").click()
loginID = browser.find_element_by_id("username").send_keys('jweinreb')
loginPass = browser.find_element_by_id("password").send_keys('55shadow')
loginReady = browser.find_element_by_class_name("login_submit")
loginReady.submit()

{% endhighlight %}

Now that we're logged in, let's choose a country and start scraping articles about it. In our example, we'll use Malaysia, which has been the subject of a lot of interesting news over the past few years, including the 1MDB scandal and the tragic disappearance of Malaysian Airlines flight 370. Remember that we've just logged in with our credentials, so to make sure we can input search terms without the code crashing, we'll need to first invoke a wait command. Here, we're giving the page 10 seconds to load before we try to locate the search box. Once we've found it, we can enter the word 'Malaysia' into the search box through our Python script. 

{% highlight python %}

## Basic search: articles containing Malaysia 
WebDriverWait(browser, 10).until(
        EC.presence_of_element_located((By.ID, "globalHatSearchInput"))
    )

search_box = browser.find_element_by_id("globalHatSearchInput")
search_box.clear()
search_box.send_keys('Malaysia') # Input search keyword
WebDriverWait(browser, 3)
search_req = browser.find_element_by_css_selector('.button-search').click()

{% endhighlight %}

If you're using Firefox (as opposed to Chrome), there will be an annoying cookies disclaimer banner that interferes with further scraping until you close it. So let's take care of that:

{% highlight python %}

## Close cookie policy if needed
try:
    browser.find_element_by_class_name("close").click()
except NoSuchElementException:
    print('Cookie agreement already acknowledged')
    
{% endhighlight %}




