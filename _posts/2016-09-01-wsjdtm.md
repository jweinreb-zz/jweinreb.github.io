---
layout: post
title:  "Parsing DTMs from the Wall Street Journal"
date:   2016-09-01
categories: post
---

A longstanding interest of mine is how investors respond to political instability -- like news of corruption scandals, gridlock, etc. Since the Wall Street Journal is a good financial newspaper of record, I'm wondering if the content of WSJ articles can help predict fluctuations in bonds and equities, when combined with monthly economic indicators like industrial production, building permits, etc. 

As a first step towards finding out, I wanted to make a script to web-scrape WSJ articles about a given country to create a document term matrix (DTM). Since the WSJ is a dynamic site, we're going to have to use `selenium` here. 

Start by importing a whole bunch of `selenium` bells and whistles, along with standard ML stuff and some processing tools from `nltk`. 

{% highlight python %}
import os, re, csv, nltk, datetime
import pandas as pd
import numpy as np

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from urllib.request import urlopen
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from unidecode import unidecode

#  Import stemmers and dictionaries for stop words
snowball = nltk.stem.SnowballStemmer('english')
lancaster = nltk.stem.LancasterStemmer()
stop_words = urlopen('http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop').read().decode("utf-8").split("\n")
stop_words = set(stop_words)

{% endhighlight %}

We'll also borrow a function from <a href = "https://github.com/andrewzc/python-wsj">andrewcz</a> 

def getPageUrl(elementLinks):
    extractLinks = []
    for element in elementLinks:
        links = element.get_attribute('href')
        extractLinks.append(links)
    return(extractLinks)




